"""
ETF ì§ˆì˜ì‘ë‹µ ì±—ë´‡ - 2ì£¼ì°¨ í”„ë¡œí† íƒ€ì…

LLM ê¸°ë°˜ ETF ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ
- RAG íŒŒì´í”„ë¼ì¸: LangChain + FAISS (Vector DB)
- LLM: OpenAI GPT-4o
- UI: Streamlit

ë©˜í†  í”¼ë“œë°± ë°˜ì˜ì‚¬í•­:
1. ì„¸ì…˜ ê¸°ë°˜ ëŒ€í™” ê¸°ë¡ (st.session_state)
2. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (ì‹¤ì‹œê°„ ë‹µë³€ ìƒì„±)
3. API ì˜ˆì™¸ ì²˜ë¦¬ ê°•í™” (RateLimitError, APIConnectionError ë“±)
4. ì¸ë¼ì¸ ì¶œì²˜ í‘œì‹œ ([ETF-001] í˜•ì‹)
5. ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘ (ì¢‹ì•„ìš”/ì‹«ì–´ìš”)
6. Edge Case ì²˜ë¦¬ (ê²€ìƒ‰ ê²°ê³¼ ì—†ì„ ë•Œ ëª…ì‹œì  ì•ˆë‚´)
"""

import os
import json
import time
from datetime import datetime
from typing import List, Tuple

import streamlit as st
from openai import OpenAI, APIError, RateLimitError, APIConnectionError

# LangChain imports
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document

# -------------------------------------------------------------------
# 0. í™˜ê²½ ì„¤ì •
# -------------------------------------------------------------------
def init_openai_client():
    """OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        st.error("í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        st.info("í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì •í•˜ì„¸ìš”: export OPENAI_API_KEY='your-api-key'")
        st.stop()
    return OpenAI(api_key=api_key)

# -------------------------------------------------------------------
# 1. ETF ë°ì´í„° ë¡œë“œ ë° ë²¡í„° DB ì´ˆê¸°í™”
# -------------------------------------------------------------------
@st.cache_resource
def load_etf_data() -> List[dict]:
    """ETF ë°ì´í„° ë¡œë“œ"""
    data_path = os.path.join(os.path.dirname(__file__), "data", "etf_data.json")
    with open(data_path, "r", encoding="utf-8") as f:
        return json.load(f)

@st.cache_resource
def init_vector_db():
    """FAISS ë²¡í„° DB ì´ˆê¸°í™”"""
    etf_data = load_etf_data()

    # ETF ë°ì´í„°ë¥¼ Document ê°ì²´ë¡œ ë³€í™˜
    documents = []
    for etf in etf_data:
        # ëª¨ë“  ì •ë³´ë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
        content = f"""
ETF ID: {etf['id']}
ìƒí’ˆëª…: {etf['name']} ({etf['ticker']})
ì¹´í…Œê³ ë¦¬: {etf['category']}
ì¶”ì¢…ì§€ìˆ˜: {etf['index']}
ìš´ìš©ì‚¬: {etf['asset_manager']}
ì´ë³´ìˆ˜: {etf['total_expense_ratio']}
ìˆœìì‚°ê°€ì¹˜(NAV): {etf['nav']}
ìˆœìì‚°ì´ì•¡(AUM): {etf['aum']}
ìƒì¥ì¼: {etf['listing_date']}
ì„¤ëª…: {etf['description']}
ìœ„í—˜ë“±ê¸‰: {etf['risk_level']}
íˆ¬ìì „ëµ: {etf['investment_strategy']}
ì£¼ìš” ë³´ìœ ì¢…ëª©: {', '.join(etf['top_holdings'])}
ë°°ë‹¹ì •ì±…: {etf['dividend_policy']}
ì¶”ì ì˜¤ì°¨: {etf['tracking_error']}
íˆ¬ìì ìœ ì˜ì‚¬í•­: {etf['investor_caution']}
"""
        doc = Document(
            page_content=content,
            metadata={"id": etf["id"], "name": etf["name"], "ticker": etf["ticker"]}
        )
        documents.append(doc)

    # OpenAI ì„ë² ë”©ìœ¼ë¡œ FAISS ë²¡í„° DB ìƒì„±
    embeddings = OpenAIEmbeddings()
    vectorstore = FAISS.from_documents(
        documents=documents,
        embedding=embeddings
    )

    return vectorstore

# -------------------------------------------------------------------
# 2. RAG ê²€ìƒ‰ í•¨ìˆ˜
# -------------------------------------------------------------------
def retrieve_relevant_docs(vectorstore, query: str, k: int = 3) -> Tuple[str, List[dict]]:
    """
    ë²¡í„° DBì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰

    Returns:
        context: ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš© (ë¬¸ìì—´)
        sources: ì¶œì²˜ ì •ë³´ ë¦¬ìŠ¤íŠ¸
    """
    results = vectorstore.similarity_search_with_score(query, k=k)

    # ìœ ì‚¬ë„ ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§ (threshold: 1.5 ì´í•˜ë§Œ ì‚¬ìš©)
    filtered_results = [(doc, score) for doc, score in results if score < 1.5]

    if not filtered_results:
        # Edge Case: ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°
        return None, []

    context_parts = []
    sources = []

    for doc, score in filtered_results:
        context_parts.append(f"[{doc.metadata['id']}] {doc.page_content}")
        sources.append({
            "id": doc.metadata["id"],
            "name": doc.metadata["name"],
            "ticker": doc.metadata["ticker"],
            "relevance_score": round(1 - score/2, 2)  # ì ìˆ˜ë¥¼ 0~1 ë²”ìœ„ë¡œ ë³€í™˜
        })

    context = "\n\n---\n\n".join(context_parts)
    return context, sources

# -------------------------------------------------------------------
# 3. LLM í˜¸ì¶œ í•¨ìˆ˜ (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)
# -------------------------------------------------------------------
def call_llm_streaming(client: OpenAI, context: str, question: str, chat_history: list):
    """
    OpenAI API ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ

    ë©˜í†  í”¼ë“œë°± ë°˜ì˜:
    - ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µìœ¼ë¡œ UX ê°œì„ 
    - ì˜ˆì™¸ ì²˜ë¦¬ ê°•í™”
    - ëŒ€í™” íˆìŠ¤í† ë¦¬ ë°˜ì˜
    """
    system_prompt = """ë„ˆëŠ” ETF íˆ¬ì ì „ë¬¸ ìƒë‹´ì‚¬ì•¼. ë‹¤ìŒ ê·œì¹™ì„ ë°˜ë“œì‹œ ì§€ì¼œ:

1. ì—­í• (Role): ETF íˆ¬ì ì •ë³´ë¥¼ ì •í™•í•˜ê²Œ ì œê³µí•˜ëŠ” ì „ë¬¸ê°€
2. ë§¥ë½(Context): ì œê³µëœ ETF ë¬¸ì„œ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€
3. ëª©í‘œ(Goal): íˆ¬ììê°€ ETF ìƒí’ˆì„ ì´í•´í•˜ê³  ì ì ˆí•œ íˆ¬ì ê²°ì •ì„ ë‚´ë¦´ ìˆ˜ ìˆë„ë¡ ë„ì›€
4. ì œì•½ì¡°ê±´(Constraint):
   - ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  "í•´ë‹¹ ì •ë³´ëŠ” ì œê³µëœ ë¬¸ì„œì— ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•´
   - ë‹µë³€ ì¤‘ íŠ¹ì • ETF ì •ë³´ë¥¼ ì¸ìš©í•  ë•ŒëŠ” ë°˜ë“œì‹œ [ETF-001] í˜•ì‹ìœ¼ë¡œ ì¶œì²˜ë¥¼ í‘œì‹œí•´
   - íˆ¬ì ê¶Œìœ ê°€ ì•„ë‹Œ ì •ë³´ ì œê³µì„ì„ ëª…ì‹œí•´
   - í•œêµ­ì–´ë¡œ ë‹µë³€í•´
   - 3~5ë¬¸ë‹¨ ì´ë‚´ë¡œ í•µì‹¬ ìœ„ì£¼ë¡œ ì„¤ëª…í•´

5. ì¶œë ¥ í˜•ì‹:
   - ë§ˆì§€ë§‰ì— "ğŸ“ ì°¸ê³  ETF" ì„¹ì…˜ì„ ë§Œë“¤ì–´ ì‚¬ìš©í•œ ETF IDë¥¼ bulletìœ¼ë¡œ ì •ë¦¬í•´
   - íˆ¬ìì ìœ ì˜ì‚¬í•­ì´ ìˆìœ¼ë©´ "âš ï¸ íˆ¬ì ìœ ì˜ì‚¬í•­" ì„¹ì…˜ì— í¬í•¨í•´
"""

    # ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ë©”ì‹œì§€ì— í¬í•¨
    messages = [{"role": "system", "content": system_prompt}]

    # ìµœê·¼ 5ê°œì˜ ëŒ€í™”ë§Œ í¬í•¨ (ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ê´€ë¦¬)
    for msg in chat_history[-10:]:
        messages.append({"role": msg["role"], "content": msg["content"]})

    # í˜„ì¬ ì§ˆë¬¸ê³¼ ì»¨í…ìŠ¤íŠ¸
    if context:
        user_message = f"""[ê²€ìƒ‰ëœ ETF ë¬¸ì„œ]
{context}

[ì‚¬ìš©ì ì§ˆë¬¸]
{question}

ìœ„ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì¤˜. ë‹µë³€ ì‹œ ì¶œì²˜ë¥¼ [ETF-XXX] í˜•ì‹ìœ¼ë¡œ í‘œì‹œí•´."""
    else:
        user_message = f"""[ì‹œìŠ¤í…œ ì•Œë¦¼] ì§ˆë¬¸ê³¼ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ETF ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.

[ì‚¬ìš©ì ì§ˆë¬¸]
{question}

ì¼ë°˜ì ì¸ ETF ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ë˜, "ì œê³µëœ ETF ë°ì´í„°ì—ì„œëŠ” ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤"ë¼ê³  ë¨¼ì € ì•ˆë‚´í•´ì¤˜."""

    messages.append({"role": "user", "content": user_message})

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=messages,
            temperature=0.3,  # ì‚¬ì‹¤ ê¸°ë°˜ ë‹µë³€ì„ ìœ„í•´ ë‚®ì€ temperature ì‚¬ìš©
            stream=True,
            timeout=60
        )
        return response

    except RateLimitError:
        st.error("âš ï¸ API í˜¸ì¶œ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        return None
    except APIConnectionError:
        st.error("âš ï¸ ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return None
    except APIError as e:
        st.error(f"âš ï¸ OpenAI API ì˜¤ë¥˜: {str(e)}")
        return None
    except Exception as e:
        st.error(f"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        return None

# -------------------------------------------------------------------
# 4. ë¡œê¹… í•¨ìˆ˜
# -------------------------------------------------------------------
def log_interaction(question: str, answer: str, sources: list, feedback: str = None):
    """
    ì§ˆì˜ì‘ë‹µ ë¡œê·¸ ì €ì¥
    - í”„ë¡¬í”„íŠ¸ íŠœë‹ ë° ì„œë¹„ìŠ¤ ê°œì„ ì„ ìœ„í•œ ë°ì´í„° ìˆ˜ì§‘
    """
    log_dir = os.path.join(os.path.dirname(__file__), "logs")
    os.makedirs(log_dir, exist_ok=True)

    log_file = os.path.join(log_dir, "chat_log.jsonl")

    log_entry = {
        "timestamp": datetime.now().isoformat(),
        "question": question,
        "answer": answer,
        "sources": [s["id"] for s in sources],
        "feedback": feedback
    }

    with open(log_file, "a", encoding="utf-8") as f:
        f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")

def log_feedback(question: str, answer: str, feedback: str):
    """ì‚¬ìš©ì í”¼ë“œë°± ë¡œê·¸"""
    log_dir = os.path.join(os.path.dirname(__file__), "logs")
    os.makedirs(log_dir, exist_ok=True)

    feedback_file = os.path.join(log_dir, "feedback_log.jsonl")

    entry = {
        "timestamp": datetime.now().isoformat(),
        "question": question,
        "answer": answer[:200] + "..." if len(answer) > 200 else answer,
        "feedback": feedback
    }

    with open(feedback_file, "a", encoding="utf-8") as f:
        f.write(json.dumps(entry, ensure_ascii=False) + "\n")

# -------------------------------------------------------------------
# 5. Streamlit UI
# -------------------------------------------------------------------
def main():
    # í˜ì´ì§€ ì„¤ì •
    st.set_page_config(
        page_title="ETF ì§ˆì˜ì‘ë‹µ ì±—ë´‡",
        page_icon="ğŸ“ˆ",
        layout="wide"
    )

    # í—¤ë”
    st.title("ğŸ“ˆ ETF ì§ˆì˜ì‘ë‹µ ì±—ë´‡")
    st.caption("LLM ê¸°ë°˜ ETF íˆ¬ì ì •ë³´ ê²€ìƒ‰ ì‹œìŠ¤í…œ | 2ì£¼ì°¨ í”„ë¡œí† íƒ€ì…")

    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("â„¹ï¸ ì„œë¹„ìŠ¤ ì•ˆë‚´")
        st.markdown("""
        ì´ ì±—ë´‡ì€ **ETF íˆ¬ì ì •ë³´**ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

        **ì£¼ìš” ê¸°ëŠ¥:**
        - ETF ìƒí’ˆ ì •ë³´ ê²€ìƒ‰
        - íˆ¬ì ì „ëµ ì„¤ëª…
        - ìœ„í—˜ë„/ìˆ˜ìˆ˜ë£Œ ë¹„êµ
        - ë°°ë‹¹ ì •ì±… ì•ˆë‚´

        **ì§€ì› ETF:**
        - êµ­ë‚´ ì£¼ì‹í˜• (KODEX 200 ë“±)
        - í•´ì™¸ ì£¼ì‹í˜• (S&P500, ë‚˜ìŠ¤ë‹¥100)
        - ì„¹í„°/í…Œë§ˆí˜• (2ì°¨ì „ì§€, ì „ê¸°ì°¨)
        - ì±„ê¶Œí˜• (ë‹¨ê¸°ì±„ê¶Œ)
        - ë°°ë‹¹í˜•, ì¸ë²„ìŠ¤í˜•
        """)

        st.divider()

        st.header("ğŸ“Š ETF ëª©ë¡")
        etf_data = load_etf_data()
        for etf in etf_data:
            with st.expander(f"{etf['name']} ({etf['ticker']})"):
                st.write(f"**ì¹´í…Œê³ ë¦¬:** {etf['category']}")
                st.write(f"**ìœ„í—˜ë“±ê¸‰:** {etf['risk_level']}")
                st.write(f"**ì´ë³´ìˆ˜:** {etf['total_expense_ratio']}")

        st.divider()

        st.warning("""
        âš ï¸ **íˆ¬ì ìœ ì˜ì‚¬í•­**

        ë³¸ ì„œë¹„ìŠ¤ëŠ” ì •ë³´ ì œê³µ ëª©ì ì´ë©°,
        íˆ¬ì ê¶Œìœ ê°€ ì•„ë‹™ë‹ˆë‹¤.
        íˆ¬ì ê²°ì •ì€ ë³¸ì¸ì˜ íŒë‹¨ê³¼
        ì±…ì„ í•˜ì— ì´ë£¨ì–´ì ¸ì•¼ í•©ë‹ˆë‹¤.
        """)

    # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    client = init_openai_client()

    # ë²¡í„° DB ì´ˆê¸°í™”
    with st.spinner("ETF ë°ì´í„°ë² ì´ìŠ¤ ë¡œë”© ì¤‘..."):
        vectorstore = init_vector_db()

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ë©˜í†  í”¼ë“œë°±: ì„¸ì…˜ ê¸°ë°˜ ëŒ€í™” ê¸°ë¡)
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "last_sources" not in st.session_state:
        st.session_state.last_sources = []
    if "last_answer" not in st.session_state:
        st.session_state.last_answer = ""
    if "last_question" not in st.session_state:
        st.session_state.last_question = ""

    # ëŒ€í™” íˆìŠ¤í† ë¦¬ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ì˜ˆì‹œ ì§ˆë¬¸ ë²„íŠ¼
    if not st.session_state.messages:
        st.markdown("### ğŸ’¡ ì´ëŸ° ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”:")
        col1, col2 = st.columns(2)

        example_questions = [
            "KODEX 200 ETFì— ëŒ€í•´ ì•Œë ¤ì¤˜",
            "ë¯¸êµ­ ì£¼ì‹ì— íˆ¬ìí•˜ëŠ” ETF ì¶”ì²œí•´ì¤˜",
            "2ì°¨ì „ì§€ ê´€ë ¨ ETFì˜ ìœ„í—˜ë„ëŠ”?",
            "ë°°ë‹¹ ìˆ˜ìµë¥ ì´ ë†’ì€ ETFëŠ”?"
        ]

        with col1:
            if st.button(example_questions[0], use_container_width=True):
                st.session_state.example_q = example_questions[0]
                st.rerun()
            if st.button(example_questions[2], use_container_width=True):
                st.session_state.example_q = example_questions[2]
                st.rerun()

        with col2:
            if st.button(example_questions[1], use_container_width=True):
                st.session_state.example_q = example_questions[1]
                st.rerun()
            if st.button(example_questions[3], use_container_width=True):
                st.session_state.example_q = example_questions[3]
                st.rerun()

    # ì˜ˆì‹œ ì§ˆë¬¸ ì²˜ë¦¬
    example_question = st.session_state.pop("example_q", None)

    # ì±„íŒ… ì…ë ¥
    user_input = st.chat_input("ETFì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”...")

    # ì…ë ¥ ì²˜ë¦¬ (ì§ì ‘ ì…ë ¥ ë˜ëŠ” ì˜ˆì‹œ ì§ˆë¬¸)
    question = example_question or user_input

    if question:
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": question})
        with st.chat_message("user"):
            st.markdown(question)

        # ë‹µë³€ ìƒì„±
        with st.chat_message("assistant"):
            # 1. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
            context, sources = retrieve_relevant_docs(vectorstore, question)
            st.session_state.last_sources = sources
            st.session_state.last_question = question

            # 2. LLM ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
            response_stream = call_llm_streaming(
                client, context, question, st.session_state.messages
            )

            if response_stream:
                # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ í‘œì‹œ (ë©˜í†  í”¼ë“œë°±: ìŠ¤íŠ¸ë¦¬ë° êµ¬í˜„)
                answer_placeholder = st.empty()
                full_response = ""

                for chunk in response_stream:
                    if chunk.choices[0].delta.content:
                        full_response += chunk.choices[0].delta.content
                        answer_placeholder.markdown(full_response + "â–Œ")

                answer_placeholder.markdown(full_response)
                st.session_state.last_answer = full_response

                # ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": full_response
                })

                # ì°¸ê³  ETF í‘œì‹œ
                if sources:
                    st.divider()
                    st.markdown("**ğŸ” ê²€ìƒ‰ëœ ETF ì •ë³´:**")
                    for src in sources:
                        st.write(f"- **{src['id']}** {src['name']} ({src['ticker']}) - ê´€ë ¨ë„: {src['relevance_score']:.0%}")

                # ë¡œê·¸ ì €ì¥
                log_interaction(question, full_response, sources)

    # í”¼ë“œë°± ë²„íŠ¼ (ë©˜í†  í”¼ë“œë°±: ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘)
    if st.session_state.last_answer:
        st.divider()
        col1, col2, col3 = st.columns([1, 1, 4])

        with col1:
            if st.button("ğŸ‘ ë„ì›€ë¨", key="feedback_positive"):
                log_feedback(
                    st.session_state.last_question,
                    st.session_state.last_answer,
                    "positive"
                )
                st.success("í”¼ë“œë°± ê°ì‚¬í•©ë‹ˆë‹¤!")

        with col2:
            if st.button("ğŸ‘ ë³„ë¡œ", key="feedback_negative"):
                log_feedback(
                    st.session_state.last_question,
                    st.session_state.last_answer,
                    "negative"
                )
                st.info("ê°œì„ ì— ì°¸ê³ í•˜ê² ìŠµë‹ˆë‹¤!")

    # ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼
    if st.session_state.messages:
        if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”"):
            st.session_state.messages = []
            st.session_state.last_sources = []
            st.session_state.last_answer = ""
            st.session_state.last_question = ""
            st.rerun()

if __name__ == "__main__":
    main()
