import os
import time
import streamlit as st
from openai import OpenAI

# """
# Tech-GPT ê¸°ìˆ ë¬¸ì˜ ì—ì´ì „íŠ¸ (2ì£¼ì°¨ í”„ë¡œí† íƒ€ì…)

# - ì‚¬ë‚´ ê¸°ìˆ  ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ Q&A ë°ëª¨
# - Streamlit + OpenAI API ë¡œ êµ¬ì„±
# - 3ì£¼ì°¨ì—ì„œ: ë²¡í„°DB, í´ë¼ìš°ë“œ ë°°í¬, ëª¨ë‹ˆí„°ë§ìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥
# """

# -------------------------------------------------------------------
# 0. OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
# -------------------------------------------------------------------
# í™˜ê²½ë³€ìˆ˜ì— OPENAI_API_KEY ë¥¼ ì„¤ì •í•´ë‘ì—ˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
#   ì˜ˆ) Linux / macOS:
#       export OPENAI_API_KEY="sk-xxx..."
#
#   Windows (PowerShell):
#       setx OPENAI_API_KEY "sk-xxx..."
#
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise RuntimeError("í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEY ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

client = OpenAI(api_key=api_key)

# -------------------------------------------------------------------
# 1. Streamlit ê¸°ë³¸ ì„¤ì •
# -------------------------------------------------------------------
st.set_page_config(page_title="Tech-GPT Prototype", page_icon="ğŸ¤–")

st.title("ğŸ¤– Tech-GPT ê¸°ìˆ ë¬¸ì˜ ì—ì´ì „íŠ¸ (2ì£¼ì°¨ í”„ë¡œí† íƒ€ì…)")
st.caption("ì‚¬ë‚´ ê¸°ìˆ  ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ì—ì´ì „íŠ¸ â€“ 2ì£¼ì°¨ ë°ëª¨ ë²„ì „")

st.markdown(
    """
ì´ ë°ëª¨ëŠ” **2ì£¼ì°¨ ê³¼ì œìš© í”„ë¡œí† íƒ€ì…**ìœ¼ë¡œ êµ¬í˜„ëœ ë²„ì „ì…ë‹ˆë‹¤.  
ì‹¤ì œ ì‚¬ë‚´ ë¬¸ì„œ ëŒ€ì‹ , ì•„ë˜ `DOCUMENTS` ë¦¬ìŠ¤íŠ¸ì— ì •ì˜ëœ ì˜ˆì‹œ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤.

3ì£¼ì°¨ì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì´ í™•ì¥ ê°€ëŠ¥í•©ë‹ˆë‹¤.

- ë²¡í„° DB(Chroma, FAISS ë“±) ì—°ë™
- í´ë¼ìš°ë“œ ë°°í¬ (GCP Cloud Run, AWS App Runner ë“±)
- ëª¨ë‹ˆí„°ë§/ë¡œê¹… ëŒ€ì‹œë³´ë“œ ê°•í™”
"""
)

# -------------------------------------------------------------------
# 2. ì„ì‹œ ê¸°ìˆ  ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ (í–¥í›„ ë²¡í„°DBë¡œ êµì²´ë  ë¶€ë¶„)
# -------------------------------------------------------------------
DOCUMENTS = [
    {
        "id": "DOC-001",
        "title": "LLM ê¸°ë°˜ RAG ì•„í‚¤í…ì²˜ ê°œìš”",
        "content": (
            "RAGëŠ” ê²€ìƒ‰ ê¸°ë°˜ìœ¼ë¡œ ì™¸ë¶€ ì§€ì‹ì„ LLMì— ì£¼ì…í•˜ì—¬ í™˜ê°ì„ ì¤„ì´ê³  "
            "ìµœì‹ ì„±ì„ í™•ë³´í•˜ëŠ” ë°©ë²•ì´ë‹¤. ê²€ìƒ‰ ë‹¨ê³„ì—ì„œëŠ” ì‚¬ìš©ì ì§ˆë¬¸ì„ ì„ë² ë”©í•˜ì—¬ "
            "ë²¡í„° DBì—ì„œ ìœ ì‚¬ë„ê°€ ë†’ì€ ë¬¸ì„œ ì¡°ê°ì„ ì¡°íšŒí•˜ê³ , ìƒì„± ë‹¨ê³„ì—ì„œëŠ” "
            "í•´ë‹¹ ë¬¸ì„œë“¤ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ LLMì— í•¨ê»˜ ì „ë‹¬í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•œë‹¤."
        ),
    },
    {
        "id": "DOC-002",
        "title": "í´ë¼ìš°ë“œ ë°°í¬ ê°€ì´ë“œ (AWS/GCP/Azure) ìš”ì•½",
        "content": (
            "LLM ì„œë¹„ìŠ¤ë¥¼ í´ë¼ìš°ë“œì— ë°°í¬í•  ë•ŒëŠ” ë™ì‹œì ‘ì†ì ìˆ˜, ì‘ë‹µì†ë„, ë¹„ìš©, ë³´ì•ˆì„ "
            "ìš°ì„ ì ìœ¼ë¡œ ê³ ë ¤í•´ì•¼ í•œë‹¤. ì´ˆê¸°ì— ê´€ë¦¬í˜• ì»¨í…Œì´ë„ˆ ì„œë¹„ìŠ¤(AWS App Runner, "
            "GCP Cloud Run, Azure Container Apps ë“±)ë¥¼ í™œìš©í•˜ë©´ ì¸í”„ë¼ ê´€ë¦¬ ë¶€ë‹´ì„ ì¤„ì¼ ìˆ˜ ìˆë‹¤."
        ),
    },
    {
        "id": "DOC-003",
        "title": "í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê¸°ë³¸ ì›ì¹™",
        "content": (
            "í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì—ì„œëŠ” ì—­í• (Role), ë§¥ë½(Context), ëª©í‘œ(Goal), ì œì•½ì¡°ê±´(Constraint)ì„ "
            "ëª…í™•í•˜ê²Œ ì£¼ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤. ì¶œë ¥ í˜•ì‹(Output)ê³¼ ê¸°ëŒ€ ìˆ˜ì¤€(Expectation)ì„ í•¨ê»˜ ì§€ì •í•˜ë©´ "
            "ë³´ë‹¤ ì¼ê´€ëœ í’ˆì§ˆì˜ ì‘ë‹µì„ ì–»ì„ ìˆ˜ ìˆë‹¤."
        ),
    },
]


def build_context(user_question: str):
    """
    ì•„ì£¼ ê°„ë‹¨í•œ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸ í…ìŠ¤íŠ¸ë¥¼ í™•ì¸í•´ì„œ
    ê´€ë ¨ ìˆì–´ ë³´ì´ëŠ” ë¬¸ì„œë¥¼ ê³¨ë¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë§Œë“œëŠ” í•¨ìˆ˜.

    2ì£¼ì°¨ ê³¼ì œì—ì„œëŠ” 'í‚¤ì›Œë“œ ê¸°ë°˜ if ë¬¸' ì •ë„ë¡œë§Œ êµ¬í˜„í•˜ê³ ,
    3ì£¼ì°¨ì—ì„œ ë²¡í„°DB / ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰ìœ¼ë¡œ í™•ì¥í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤.
    """
    selected_docs = []

    q = user_question.lower()

    # í‚¤ì›Œë“œ ê¸°ë°˜ ê°„ë‹¨ ë§¤ì¹­ ë¡œì§ (ë°ëª¨ ìˆ˜ì¤€)
    if "rag" in q or "ê²€ìƒ‰" in q or "ì§€ì‹" in q:
        selected_docs.append(DOCUMENTS[0])
    if "ë°°í¬" in q or "í´ë¼ìš°ë“œ" in q or "cloud" in q:
        selected_docs.append(DOCUMENTS[1])
    if "í”„ë¡¬í”„íŠ¸" in q or "prompt" in q:
        selected_docs.append(DOCUMENTS[2])

    # ì•„ë¬´ ì¡°ê±´ë„ ì•ˆ ê±¸ë¦¬ë©´ ì „ì²´ ë¬¸ì„œ ì‚¬ìš©
    if not selected_docs:
        selected_docs = DOCUMENTS

    context_text = "\n\n---\n\n".join(
        f"[{d['id']}] {d['title']}\n{d['content']}" for d in selected_docs
    )
    return context_text, selected_docs


def call_llm(context: str, question: str) -> str:
    """
    OpenAI Chat Completions API ë¥¼ í˜¸ì¶œí•˜ì—¬
    RAG ìŠ¤íƒ€ì¼ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜.
    """
    system_prompt = (
        "ë„ˆëŠ” ì‚¬ë‚´ ê¸°ìˆ  ë¬¸ì„œë¥¼ ê°€ì¥ ì˜ ì´í•´í•˜ëŠ” ì‹œë‹ˆì–´ ì—”ì§€ë‹ˆì–´ì•¼. "
        "í•­ìƒ ì‚¬ì‹¤ ê¸°ë°˜ìœ¼ë¡œë§Œ ëŒ€ë‹µí•˜ê³ , ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  "
        "'í•´ë‹¹ ë¬¸ì„œ ê¸°ì¤€ìœ¼ë¡œëŠ” ë‹µë³€í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤'ë¼ê³  ë‹µí•´."
    )

    user_prompt = f"""
[ì»¨í…ìŠ¤íŠ¸(ë¬¸ì„œ ë‚´ìš©)]
{context}

[ì‚¬ìš©ì ì§ˆë¬¸]
{question}

[ì¶œë ¥ ì¡°ê±´]
- í•œêµ­ì–´ë¡œ ë‹µë³€í•´.
- 3~5ë¬¸ë‹¨ ì´ë‚´ë¡œ, ì¤‘ìš”í•œ ë‚´ìš©ì„ ìœ„ì£¼ë¡œ ì„¤ëª…í•´.
- ë§ˆì§€ë§‰ì— "ğŸ“ ì°¸ê³  ë¬¸ì„œ" ì„¹ì…˜ì„ ë§Œë“¤ì–´, ì‚¬ìš©í•œ ë¬¸ì„œ IDë¥¼ bulletìœ¼ë¡œ ì •ë¦¬í•´ì¤˜.
"""

    response = client.chat.completions.create(
        model="gpt-4.1-mini",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        temperature=0.2,
    )

    return response.choices[0].message.content


def log_interaction(question: str, answer: str):
    """
    ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œê¹….
    (íƒ€ì„ìŠ¤íƒ¬í”„, ì§ˆë¬¸, ë‹µë³€)
    """
    ts = time.strftime("%Y-%m-%d %H:%M:%S")
    log_line = f"[{ts}]\nQ: {question}\nA: {answer}\n\n"

    with open("techgpt_log.txt", "a", encoding="utf-8") as f:
        f.write(log_line)


# -------------------------------------------------------------------
# 3. Streamlit UI
# -------------------------------------------------------------------
st.subheader("ì§ˆë¬¸ ì…ë ¥")

user_question = st.text_area(
    "ê¸°ìˆ  ê´€ë ¨í•´ì„œ ê¶ê¸ˆí•œ ë‚´ìš©ì„ ìì—°ì–´ë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”.",
    placeholder="ì˜ˆ) RAG ì•„í‚¤í…ì²˜ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•´ì¤˜\nì˜ˆ) LLM ì„œë¹„ìŠ¤ë¥¼ í´ë¼ìš°ë“œì— ë°°í¬í•  ë•Œ ë¬´ì—‡ì„ ê³ ë ¤í•´ì•¼ í•´?",
    height=120,
)

col1, col2 = st.columns([1, 3])

with col1:
    ask_button = st.button("ì§ˆë¬¸í•˜ê¸°", type="primary")

with col2:
    st.info("ì§ˆë¬¸ì„ ì…ë ¥í•œ ë’¤ **ì§ˆë¬¸í•˜ê¸°** ë²„íŠ¼ì„ ëˆ„ë¥´ë©´, Tech-GPTê°€ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ ë‹µë³€í•©ë‹ˆë‹¤.")

if ask_button:
    if not user_question.strip():
        st.warning("ë¨¼ì € ì§ˆë¬¸ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
    else:
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            context, used_docs = build_context(user_question)
            answer = call_llm(context, user_question)
            log_interaction(user_question, answer)

        st.markdown("### âœ… ë‹µë³€")
        st.write(answer)

        st.markdown("### ğŸ” ì‹œìŠ¤í…œì´ ì°¸ê³ í•œ ë¬¸ì„œ ëª©ë¡")
        for d in used_docs:
            st.write(f"- **{d['id']}** : {d['title']}")

        st.success("2ì£¼ì°¨ ê³¼ì œìš© í”„ë¡œí† íƒ€ì… ë™ì‘ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
